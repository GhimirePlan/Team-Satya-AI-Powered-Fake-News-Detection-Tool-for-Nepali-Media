import re
import scipy
import pandas as pd
import torch
import re
from nltk.tokenize import RegexpTokenizer
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle
from torch.utils.data import DataLoader, TensorDataset
import torch.nn as nn
import torch.optim as optim

class FakeNewsLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):
        super(FakeNewsLSTM, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, 
                            hidden_dim, 
                            num_layers=n_layers, 
                            bidirectional=bidirectional, 
                            dropout=dropout, 
                            batch_first=True)
        
        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        embedded = self.dropout(self.embedding(x))
        lstm_out, (hidden, cell) = self.lstm(embedded)
        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)) if self.lstm.bidirectional else hidden[-1,:,:]
        output = self.fc(hidden)
        return self.sigmoid(output)
    
class Prediction:
    def __init__(self,OC, LR, GB, RF, pt,vect,vocab):
        self.OC = OC
        self.LR = LR
        self.GB = GB
        self.RF = RF
        self.pt = pt
        self.vect = vect

        self.vocab = vocab
        self.suf = []
        self.words = [ ]
        
        with open("suf.txt", "r", encoding="utf-8-sig") as s:
            for t in s.read().split("\n"):
                self.suf.append(t)
        
        
        with open("dict.txt", "r", encoding="utf-8-sig") as s:
            for t in s.read().split("\n"):
                self.words.append(t)

    def clean_text_(self,text):
        text = re.sub(r'[^\u0900-\u097F\s]', '', text)  
        text = re.sub(r'\s+', ' ', text).strip() 
        puncts = r"$&+,:;=?@#|'<>.^*()%!-à¥¤"
        if text in list(puncts):
            return text
        else:
            text = re.sub(r"[$&+à¥¤à¥¤,:;=?@#|'<>.^*()%!-]","", text)

        return text
    tokenizer = RegexpTokenizer(r'[\u0900-\u097F]+') 

    def build_vocab(self,tokenized_texts, max_vocab_size=10000):
        all_tokens = [token for tokens in tokenized_texts for token in tokens]
        word_counts = Counter(all_tokens)
        vocab = {word: idx + 2 for idx, (word, _) in enumerate(word_counts.most_common(max_vocab_size))}
        vocab["<PAD>"] = 0  
        vocab["<UNK>"] = 1  
        return vocab

    def text_to_sequence(self,tokens, vocab):
        return [vocab.get(token, vocab["<UNK>"]) for token in tokens]

    def preprocess_data(self,df, vocab):
        tokenized_texts = []
        for text in df['text']:
            cleaned_text = self.clean_text_(text)
            tokens = self.tokenizer.tokenize(cleaned_text)
            tokenized_texts.append(tokens)
        
        sequences = [self.text_to_sequence(tokens, vocab) for tokens in tokenized_texts]
        
        return sequences


    def removeCharacters(self,s):
        tmp = s
            
        tmp = re.sub(r"[\x00\x01\x10\x15\x16\x1a\x1e\x02\x03\x05\x06\x07\x08\x17\x1f\x7f\x81\x82\x83\x84\x85\x86\x88\x8d\x8f\x92\x93\x94\x95\x96\x9d\x9e\xa0\xad\uf047\uf0a8\u09b1\uf0af\uf106\u2003\u206e\u206f\u2002\u2009\u200a\u200b\u200c\u200d\u200e\u200f\u2028\u2029\u202a\u202c\u202d\u202f\u2060\u3000\ue86c\uf000\uf001\uf002\uf020\uf026\uf02a\uf02d\uf02f\uf034\uf038\uf03a\uf03c\uf03d\uf03f\uf040\uf046\uf061\uf065\uf06c\uf06d\uf06e\uf071\uf072\uf074\uf075\uf076\uf077\uf07d\uf085\uf096\uf0a2\uf0a7\uf0ab\uf0b6\uf0b7\uf0d8\uf0da\uf0dc\uf0e0\uf0fc\uf164\uf16a\uf339\uf3c6\uf449\uf44c\uf468\uf495\uf4fd\uf527\uf602\uf60a\uf62d\uf64f\uf8ea\uf8ff\ufeff\u0604\U00100000\U00100001\U001000a5\U001000f2\U0010012e\U001001d1\U001001d3\U00100223\U0010024d\u2063\u09b1]+","", ##removing unparsed unicode char sequence
                    re.sub(r"[a-zA-Zï½-ï½šï¼¡-ï¼º\u00C0-\u024F\u1E00-\u1EFF]+","", # Supposed to be Accented Alphabets
                    re.sub(r'[ä¸€-é¾ ã-ã‚”ã‚¡-ãƒ´ãƒ¼ã€…ã€†ã€¤ãƒ¶ã‚¢ã‚¬ã‚·ã‚ºã‚¿ãƒ‹ãƒãƒ¡ãƒ«ãƒ¼ä¸‹ä¸­äºšäºäººä¾†å…¬å…±åŒºå€å¤å¸åå´å’Œå“”å“©å›½åœ‹åœ³å¡å¤šå®¢å®®å®¶å±±å²¡æˆ˜æˆ°æ–‡æ—¥æ˜¥æœ‰æœ¨æœ¬æœ´æ‘æ¥æ°‘æ²™æ·±ç›¸ç§‹ç«™ç²µç¹”è¯è•ƒè—è¥¿è¨€è©±èªè¯­è´›è¼‰é¥é™éŸ³é£¯é¦¬é©¬é®¨ê°€ê³µêµ¬êµ­ë‹¤ëŒ€ëŸ¬ë¯¼ë°”ë°•ìƒì„œì„ ìŠ¤ì–´ìš¸ìœ ì˜ì¸ì¬ì¡°ì£¼ì¶•íŒ€í‘œí”Œí•œí™”ã„©ã…ã¡êŠê£³ê°ê°„ê°ê°•ê°™ê°œê±°ê±´ê±¸ê²ƒê²Œê² ê²¨ê²¬ê²½ê³„ê³ ê³¼ê´€ê´‘êµê¶ê¶Œê·€ê·¸ê·¹ê·¼ê¸€ê¸ˆê¸‰ê¸°ê¹€ê¹Œê»´ê¼­ë‚˜ë‚œë‚¨ë‚©ë‚´ë„ˆë„‰ë„“ë„¤ë„¥ë…€ë…„ë…¸ëŠëŠ”ëŠ˜ëŠ¥ë‹ˆë‹ë‹¨ë‹´ë‹µë‹¹ë”ë„ë™ë˜ëœë©ë‘ë“œë“¤ë“±ë””ë•Œë–¨ë ë¼ëŒëë˜ë©ëŸ°ëŸ¼ëŸ½ë ˆë ¥ë¡œë¡ë¡ ë£Œë¥¼ë¦„ë¦ë¦¬ë¦­ë¦½ë§ˆë§Œë§ë§›ë§ë§¤ë§ºë©‹ë©”ë©˜ë©°ë©´ëª…ëª¨ëª©ë¬´ë¬¸ë¬¼ë¯¸ë°ë°œë°©ë°°ë²ˆë²•ë² ë³€ë³„ë³´ë³µë³¼ë´‰ë¶€ë¶„ë¸Œë¹„ë¿ì‚¬ì‚´ìƒìƒµì„¯ì„±ì„¸ì…”ì†Œì†¡ìˆ˜ì‰¬ìŠµì‹œì‹ì‹ ì‹¤ì‹­ì‹¸ì¨ì•„ì•ˆì•Šì•™ì•¡ì–‘ì–¸ì—…ì—†ì—ˆì—ì—¬ì—­ì—°ì—´ì˜ì˜¤ì™€ì™¸ìš”ìš©ìš°ìš´ì›Œì›ì›”ì› ìœ„ìœ¡ìœ¼ì€ì„ìŒì´ì¼ì„ì…ìˆìì‘ì˜ì ì¥ì €ì ì „ì ì •ì œì ¸ì¡´ì¢…ì¢‹ì¤€ì¤ì¤ì¤‘ì¦ˆì¦£ì§€ì§ì§„ì§‘ì§•ì§ ì§±ì§¸ì°¨ì°©ì°¬ì°¸ì°¾ì±…ì²œì²«ì²­ì²´ì´ˆìµœì¶”ì¶œì¹˜ì¹´ì»¬ì¼“ì½”í¬í°í´í‚¤í‚¹íƒ€íƒí„°í…œí† í†¤í†µíŠ¸íŠ¹íŠ¼í‹°íŒíŒ¬í˜í¸í‰í¬í”„í”¼í•„í•í•˜í•™í• í•¨í•©í•­í•´í–‡í–‰í–¥í—ˆí˜„í˜œí˜¸í™ˆí™•í™˜í™œí™©íšŒí›„íœ´ï¸ï¹¾ï¼ï¼Œï¼šï¼Ÿï½œï½ï½ªï½«ï¾¥ï¾§ï¾ª]+', "", ## Supposed to be Korean
                    re.sub(r'[~Â¡Â£Â¤Â¥Â¦Â§Â¨Â©ÂªÂ«Â¬Â®Â¯Â±Â²ÂµÂ¶Â¹ÂºÂ»Â¼Â½Â¾à§·à©¯à©°â€“â€”â€•â€ â€¡â€¢â€¦â€°â€¹â€ºâ„â‚ªâ‚¬â‚±â‚¹â„…â„¢â…“â†â†’âˆˆâˆ‰âˆ‘âˆ’âˆ•âˆ™âˆšâˆ›â‰ˆâ‰ â‰¡â‰¤â‹…â”â”œâ”¬â••â•–â•—â•›â•¡â•¢â•£â•¦â–‡â–‘â–“â– â–ªâ—â§¼â§½ï¼ˆï¼‰â€£ï¿¼ï¿½â˜ºâ˜¼âœŠâœ‹âœŒâœâœâœ“âœ”âœ´â†â¤â™«ğŸ‡³ğŸ‡µğŸŒ´ğŸŒ·ğŸŒ¹ğŸŒºğŸŒ»ğŸŒ½ğŸŒ¾ğŸŒ¿ğŸğŸ¤ğŸ¹ğŸ¼ğŸ¡ğŸ¼ğŸ¿ğŸğŸ’ğŸ‘ğŸ‘ˆğŸ‘‰ğŸ‘ğŸ‘¤ğŸ‘§ğŸ‘¨ğŸ‘±ğŸ’€ğŸ’ƒğŸ’ŒğŸ’ğŸ’“ğŸ’”ğŸ’•ğŸ’˜ğŸ’¥ğŸ’¦ğŸ’¿ğŸ“»ğŸ”¥ğŸ”±ğŸ•¸ğŸ–•ğŸ˜€ğŸ˜‚ğŸ˜ƒğŸ˜…ğŸ˜†ğŸ˜ˆğŸ˜‰ğŸ˜ŠğŸ˜ğŸ˜’ğŸ˜”ğŸ˜˜ğŸ˜›ğŸ˜ğŸ˜ ğŸ˜¡ğŸ˜¢ğŸ˜¥ğŸ˜¦ğŸ˜­ğŸ™ğŸ™‚ğŸ™ˆğŸ™‰ğŸ™ŠğŸ™‹ğŸ™ğŸš©ğŸš«ğŸ¤”ğŸ¤£ó †¶]+',"",  ## Symbols
                           tmp))))
                            
        tmp = re.sub(r'[Â¿Ã€ÃÃ…Ã‡ÃˆÃÃÃ“Ã–Ã—Ã˜Ã™Ã›ÃœÃÃÃ Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¶Ã·Ã¸Ã¹ÃºÃ¼Ã½Ä€ÄÄƒÄ‡ÄŒÄÄÄ‘Ä•Ä—ÄŸÄ©Ä«Ä±Å‚ÅˆÅ‹ÅŒÅÅÅ’Å“Å™ÅšÅ›Å Å¡Å©Å«Å¯Å³Å¸Å¹Å½Æ’ÇÇ§É‘É’É”É™É›É¡ÉªÉ¾ÊƒÊ‰ÊŠÊŒÊ’Ê”á¸—á¹³áº¡áº£áº¹áº»áº½áº¿á»…á»‡á»‰á»‹á»á»á»“á»¥á»§á¼€á¼á¼±á¼·á¾¶á¿¥â²â²â²“â²•â²£]+',"",
                    re.sub(r'[Ê¾Ë†ËˆËŒËË‘ÎÎ•Î£Î¬Î­Î¯Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏ‚ÏƒÏ…Ï†Ï‰ÏŒÏÏ©Ğ„Ğ†ĞĞ‘Ğ’Ğ“Ğ•Ğ–Ğ—Ğ˜Ğ™ĞšĞ›ĞœĞĞĞŸĞ Ğ¡Ğ¢Ğ£Ğ¤Ğ¥Ğ¦Ğ§Ğ¨Ğ©ĞªĞ«Ğ¬Ğ­Ğ®Ğ¯Ğ°Ğ±Ğ²Ğ³Ğ´ĞµĞ¶Ğ·Ğ¸Ğ¹ĞºĞ»Ğ¼Ğ½Ğ¾Ğ¿Ñ€ÑÑ‚ÑƒÑ„Ñ…Ñ†Ñ‡ÑˆÑŠÑ‹ÑŒÑÑÑÑ‘Ñ—ÒšÒ›Ò¡Ò§Ò¯Ò·Ó£Ô«Õ€Õ¡Õ¥ÕµÕ¶Ö€]+',"",
                           re.sub(r'[Ö´××‘×“×•×™×œ×Ÿ×¢×¤×§×¨×©×ªØŒØ’Ø›ØŸØ¡Ø¢Ø¥Ø¦Ø§Ø¨Ø©ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙ‰ÙŠÙ‹ÙÙÙÙ‘Ù’Ù”ÙªÙ«Ù¬Ù°Ù¹Ù¾Ú†ÚˆÚ‰Ú‘Ú˜ÚšÚ©Ú¬Ú¯ÚµÚºÚ¾ÛÛ†ÛŒÛÛÛ’Û“ï®ïº—ïº ï»ªï»«ï»®Û”Û›Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹ÜÜ’Ü—ÜÜ¡Ü¢ÜªÜ¬Ş€Ş„ŞˆŞ‹ŞŞ¦Ş¨Ş¬Ş°]+',"",
                           tmp)))
        
        tmp = re.sub(r'[à¦…à¦†à¦‡à¦‰à¦‹à¦à¦à¦“à¦•à¦–à¦—à¦˜à¦™à¦šà¦›à¦œà¦à¦à¦Ÿà¦ à¦¡à¦¢à¦£à¦¤à¦¥à¦¦à¦§à¦¨à¦ªà¦«à¦¬à¦­à¦®à¦¯à¦°à¦²à¦¶à¦·à¦¸à¦¹à¦¼à¦¾à¦¿à§€à§à§‚à§‡à§ˆà§‹à§à§°à§±à§³à¨…à¨†à¨‡à¨ˆà¨‰à¨Šà¨à¨à¨“à¨”à¨•à¨–à¨—à¨˜à¨™à¨šà¨›à¨œà¨à¨à¨Ÿà¨ à¨¡à¨¢à¨£à¨¤à¨¥à¨¦à¨§à¨¨à¨ªà¨«à¨¬à¨­à¨®à¨¯à¨°à¨²à¨µà¨¸à¨¹à¨¼à¨¾à¨¿à©€à©à©‚à©‡à©ˆà©‹à©Œà©à©œà©¦à©§à©¨à©©à©ªà©«à©¬à©­à©®àª•àª—àªœàªàª¤àª¦àª¨àª«àª®àª°àª·àª¸àª¾àª¿à«€à«à«‡à«‹à«à¬†à¬“à¬¡à¬¼à¬¿à®…à®‡à®•à®™à®šà®œà®Ÿà®¤à®¨à®©à®ªà®®à®¯à®°à®±à®²à®³à®´à®µà®¸à®¾à®¿à¯€à¯à¯‚à¯‡à¯ˆà¯Šà¯à°…à°—à°¤à°¥à°¨à°ªà°®à°°à°²à°¶à°¹à°¾à°¿à±à±‚à±†à±Šà±à²•à²¡à²¨à³à´‚à´…à´œà´à´ªà´¬à´®à´¯à´°à´²à´³à´¾à´¿àµàµ‹àµà¶‚à¶šà¶±à¶½à·ƒà·„à·’à¸à¸‚à¸„à¸‡à¸Šà¸à¸•à¸—à¸˜à¸™à¸šà¸›à¸Ÿà¸¡à¸¢à¸£à¸¥à¸§à¸¨à¸©à¸ªà¸«à¸­à¸°à¸±à¸²à¸³à¸´à¸µà¸¸à¹€à¹ƒà¹„à¹ˆà¹‰à¹ŒàºŠàº”àº•àº—àº™àº›àº¥àº§àºªàº°àº±àº²àº´àº»à»„à¼‹à¼à½€à½à½‚à½„à½…à½†à½‡à½‰à½Šà½‹à½Œà½à½à½à½‘à½“à½”à½•à½–à½˜à½™à½šà½›à½à½à½Ÿà½ à½¡à½¢à½£à½¤à½¥à½¦à½§à½¨à½²à½´à½ºà½¼à¾à¾’à¾£à¾©à¾«à¾±à¾²à¾µà¾·á€…á€Šá€á€”á€•á€™á€›á€¬á€¯á€±á€¶á€¹á€ºá€¼áƒáƒ’áƒ—áƒ˜áƒšáƒ›áƒ áƒ£áƒ¥áˆ›áˆ­áŠ›áŠ áá—á˜ášáŸá¶áŸ‚áŸ’á¤€áµ»à¤½àª…àª‡àªàª“àªšàª›àªàªªàª¬àª¯àª²àªµàª¶àª¹à«Œà¯†à°‚à°£à²—à²£à²¬à²°à²³à²µà²¶à³†à³Šà´‡à´à´à´à´’à´•à´–à´—à´˜à´™à´šà´Ÿà´¡à´£à´¤à´¥à´¦à´§à´¨à´«à´­à´±à´´à´µà´¶à´·à´¸à´¹àµ€àµ‚àµ†àµ‡àµˆàµŠàµ»àµ¼àµ½àµ¾à¸ˆà¸‰à¸‹à¸’à¸“à¸”à¸–à¸œà¸à¸ à¸¶à¸·à¸¹à¹à¹‚à¹†à¹‡à¹Šà¹›àºà¼ºà¼»à¾Ÿà¾¤à¾´á€—áƒ‘áƒ“áƒ”áƒ•áƒ–áƒ™áƒœáƒáƒáƒ¡áƒ¢áƒ¤áƒ¦áƒ§áƒ¨áƒ©áƒªáƒ«áƒ¬áƒ­áƒ®áƒ¯áƒ°á¦á©á¯á³á‚áá£á¬á–³á€á‚á„á…á‡á‰áŠáááá‘á’á“á”á•á–á™á›áœá á¡á¢á¬á¯á²á·á¸á¹á»á¼á½á¾áŸ€áŸáŸƒáŸ„áŸ…áŸ†áŸ‡áŸˆáŸ‰áŸŠáŸ‹áŸŒáŸáŸáŸ”áŸ•áŸ¡áŸ¥áŸ¨]+',"",
                           tmp)
        
        tmp = re.sub(r"[Â°Â³Â´Â·Â¸ÉÉ•É–É£É¦É¨É«É¯É³É´ÊÊˆÊ‹ÊÊ‘Ê°Ê²Ê½ËŠË‹Ë—Ë˜ËšËœËÌÌƒÌŠÌÌ’ÌšÌÌ¡Ì§Ì©ÌªÌ­Ì¯ÌµÌ·ÍƒÍˆÍ™Í¡Í§Í¬Î‘Î’Î”Î–Î—Î˜Î™ÎšÎ›ÎœÎÎŸÎ Î¡Î¤Î¥Î¦Î§Î©Î®Ï„ÏĞ”Ñ‰Ñ”Ñ–Ñ˜Ò«Ò³Ò¶Ó©Ó¯Ô¨Õ½Õ¿Ö°Ö²ÖµÖ·Ö¸Ö¹Ö½×€××‚×”×–×—×˜×š×›××× ×¡×¥×¦Ø”Ø˜Ø£Ø¤Ù€Ù»ÚŒÚªÛ©Û¾ß®ß°à¤„â€»â‚‹â„ƒâ„–â†‘â†©â‡„â‡’â‡¨âˆ‡âˆâˆ˜âˆâŠ•âŠ—â‹²â¼â‘£â‘¸â”€â”‚â”Œâ•˜â–ºâ–¼â—„â—Šâ—Œâ—â˜…â˜†â˜â˜‘â˜ºâ˜»â™‚â™ â™¢â™£â™¥â™¦â™ªâš â›­âœ‰â¶â·â¸â¹âºâ§âŸ²âŸ³â¦â­•ã€ã€‚ã€Šã€‹ã€Œã€ã€ã€ã€ã€‘ğŸ†šğŸ‡§ğŸ‡¬]+","",tmp)
        
        tmp = re.sub(r"[ \t]+"," ",tmp).strip()
        """
        query = tmp
        querywords = query.split()
    
        resultwords  = [word for word in querywords if word.lower() not in stopwords]
        tmp = ' '.join(resultwords)
        """
        s = tmp
            
        return s

    def clean_text(self, text, chars=None):
        puncts = r"$&+,:;=?@#|'<>.^*()%!-"
    
        if text in list(puncts):
            
            return text
        w = []
        w.append(text)
        w.append([])
        w[1] = re.findall(r"[$&+,:;=?@#|'<>.^*()%!-]", text)
        if len(w[1]) > 0:
            for p in w[1]:
                if w[0].endswith(p):
                    w[0] = w[0].removesuffix(p)
                
            
        
        return w
    def pad_sequences(self,sequences, max_length):
        padded = []
        for seq in sequences:
            if len(seq) > max_length:
                padded.append(seq[:max_length])
            else:
                padded.append(seq + [0] * (max_length - len(seq)))  
        return padded   

    def stem(self,text):
        if text in self.suf:
            return text

        w = []
        for t in text.split(" "):
            if t not in self.words:
                pattern = r'(\s*)(' + '|'.join(map(re.escape, self.suf)) + r')$'
                modified_text = re.sub(pattern, r' \2', t)
                if len(modified_text.split(" "))>1:
                    modified_text = self.stem(modified_text.split(" ")[0]) + " " + modified_text.split(" ")[1]
                w.append(modified_text)
            else:
                w.append(t)
            
        modified_text = " ".join(w)
        
        return modified_text
    def get_percent(self):
        t = self.p[0] + self.ms[0] + self.ns[0] + self.o[0] + self.op[0]
        t = t[0]
        print(f'{t/5*100:.4f}%')

    def add(self, df, text, cls):
        text = self.clean_text_(text)
        text = self.stem(text)
        temp = pd.DataFrame([[text,cls]], columns=["text", "class"])
        df = pd.concat([df,temp])
        if df["class"].value_counts()[0]>df["class"].value_counts()[1]:
            temp = df[df["class"]==1].sample(frac=1/len(df[df["class"]==1]))
            df = pd.concat([df,temp])
        elif df["class"].value_counts()[0]<df["class"].value_counts()[1]:
            temp = df[df["class"]==0].sample(frac=1/len(df[df["class"]==0]))
            df = pd.concat([df,temp])
        df = df.reset_index()
        df = df.drop(["index"], axis=1)
        
        return df

    def fit(self, dframe):
        labels = dframe['class']
        text = dframe['text']
        dframe['text'] = dframe['text'].fillna("")
        text = text.fillna("")
        self.vect.fit(text)
        v = self.vect.transform(text)
        self.OC.fit(v, labels)
        print(f'{self.OC} trained.')
        self.m.fit(v, labels)
        print(f'{self.m} trained.')
        self.LR.fit(v, labels)
        print(f'{self.LR} trained.')
        self.GB.fit(v, labels)
        print(f'{self.GB} trained.')
        tokenized_texts = [self.tokenizer.tokenize(self.clean_text_(text)) for text in dframe['text']]
        self.vocab = self.build_vocab(tokenized_texts)

        sequences = self.preprocess_data(dframe, self.vocab)
        dframe['sequences'] = sequences

        max_length = 100
        padded_sequences = self.pad_sequences(sequences, max_length)

        dframe['padded_sequences'] = padded_sequences
        sequences_tensor = torch.tensor(padded_sequences)
        labels_tensor = torch.tensor(dframe['class'].values)

        dataset = TensorDataset(sequences_tensor, labels_tensor)
        dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

        for batch in dataloader:
            inputs, labels = batch
            print(inputs, labels)

        embedding_dim = 100
        hidden_dim = 256
        output_dim = 1
        n_layers = 2
        bidirectional = True
        dropout = 0.5
        model = FakeNewsLSTM(len(self.vocab), embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)

        criterion = nn.BCELoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)

        num_epochs = 2
        for epoch in range(num_epochs):
            model.train()
            epoch_loss = 0
            for batch in dataloader:
                text, labels = batch
                optimizer.zero_grad()
                predictions = model(text).squeeze(1)
                loss = criterion(predictions, labels.float())
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            
            print(f'Epoch {epoch+1}, Loss: {epoch_loss / len(dataloader)}')
        self.pt = model
    def save_models(self):        
        with open('RF.pkl', 'wb') as f:
            pickle.dump(self.RF, f)
        with open('GB.pkl', 'wb') as f:
            pickle.dump(self.GB, f)
        with open('LR.pkl', 'wb') as f:
            pickle.dump(self.LR, f)
        with open('OC.pkl', 'wb') as f:
            pickle.dump(self.OC, f)
        with open('vect.pkl', 'wb') as f:
            pickle.dump(self.vect, f)
        with open('PT.pkl', 'wb') as f:
            pickle.dump(self.pt, f)
        with open('vocab.pkl', 'wb') as f:
            pickle.dump(self.vocab, f)

    def predict(self, s=None):
        if s==None:
            sss = input()
        else:
            sss =s 
        sss = pd.DataFrame([sss], columns=["text"])
        sss["text"] = sss["text"].apply(self.removeCharacters)
        sss["text"] = sss["text"].apply(self.stem)
        nv = self.vect.transform(sss["text"])
        #self.m.eval()
        #tc = self.m(torch.tensor(scipy.sparse.csr_matrix.todense(nv)).float())
        #ps = torch.exp(tc)
        #tk, p = ps.topk(1, dim=1)
        self.p = self.RF.predict(nv)
        self.ms = self.OC.predict(nv)
        self.ns = self.LR.predict(nv)
        self.o = self.GB.predict(nv)
        ss = self.preprocess_data(sss, self.vocab)
        sss['sequences'] = ss
        max_length = 100
        ps = self.pad_sequences(ss, max_length)
        ip = torch.tensor(ps)
        self.op = self.pt(ip)
        
        if self.ms[0]==0:
            print("FAKE")
        elif self.ms[0] == 1:
            print("REAL")
        
        if self.ns == 0:
            print("FAKE")
        else:
            print("REAL")
        if self.o == 0:
            print("FAKE")
        else:
            print("REAL")
        if self.p == 0:
            print("FAKE")
        else:
            print("REAL")
        if self.op < 0.5:
            print("FAKE")
        else:
            print("REAL")

        return [self.ms.item(), self.ns.item(), self.o.item(),self.p.item(), self.op.item()]
